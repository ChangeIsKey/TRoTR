{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cdf0b932-a4f0-4aa9-8218-122f2d32464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_uses(filename='TRoTR/data/uses.tsv', sep='\\t'):\n",
    "    tmp = list()\n",
    "    with open(filename, mode='r', encoding='utf-8') as f:\n",
    "        columns = f.readline().rstrip().split(sep)\n",
    "        for line in f.readlines():\n",
    "            tmp.append(dict(zip(columns, line.rstrip().split(sep))))\n",
    "    \n",
    "    return pd.DataFrame(tmp)\n",
    "\n",
    "def load_instances(filename, dirname='TRoTR/rounds', sep='\\t'):\n",
    "    tmp = list()\n",
    "    with open(f'{dirname}/{filename}', mode='r', encoding='utf-8') as f:\n",
    "        columns = f.readline().rstrip().split(sep) + ['dataID1', 'dataID2']\n",
    "        for line in f.readlines():\n",
    "            tmp_record = dict(zip(columns, line[:-1].split('\\t')))\n",
    "            tmp_record['dataID1'], tmp_record['dataID2'] = tmp_record['dataIDs'].split(',')\n",
    "            tmp.append(tmp_record)\n",
    "    \n",
    "    return pd.DataFrame(tmp)\n",
    "\n",
    "def load_judgments(filename, dirname='TRoTR/judgments', sep='\\t'):\n",
    "    tmp = list()\n",
    "    with open(f'{dirname}/{filename}', mode='r', encoding='utf-8') as f:\n",
    "        columns = f.readline().rstrip().split(sep)\n",
    "        for line in f.readlines():\n",
    "            tmp_record = dict(zip(columns, line.rstrip().split(sep)))\n",
    "            tmp.append(tmp_record)\n",
    "            \n",
    "    #tmp - to remove after fixing the bug\n",
    "    tmp = list()\n",
    "    with open(f'{dirname}/{filename}', mode='r', encoding='utf-8') as f:\n",
    "        columns = f.readline().rstrip().split(sep)\n",
    "        f = f.read().replace('iosakwe\\n', 'iosakwe@@@').replace('shur\\n', 'shur@@@').replace('Nisha\\n', 'Nisha@@@').replace('AndreaMariaC\\n', 'AndreaMariaC@@@').replace('\\n', '--')\n",
    "        lines = f.split('@@@')\n",
    "        for line in lines:\n",
    "            tmp_record = dict(zip(columns, line.rstrip().split(sep)))\n",
    "            tmp.append(tmp_record)\n",
    "\n",
    "    # -1: can not decide\n",
    "    df = pd.DataFrame(tmp).fillna('-1')\n",
    "    df['label'] = df['label'].apply(lambda x: x.replace('-', '-1')).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def merge_data(df_uses, df_instances, df_judgments):\n",
    "    df = df_judgments.merge(df_instances).merge(df_uses, left_on='dataID1', right_on='dataID')\n",
    "    del df['dataID']\n",
    "    del df['lemma']\n",
    "    df = df.rename(columns={column: f'{column}1' for column in ['context', 'indices_target_token', 'indices_target_sentence']})\n",
    "    df = df.merge(df_uses, left_on='dataID2', right_on='dataID')\n",
    "    del df['dataID']\n",
    "    df = df.rename(columns={column: f'{column}2' for column in ['context', 'indices_target_token', 'indices_target_sentence']})\n",
    "    \n",
    "    column_order = ['instanceID', 'dataID1', 'dataID2', 'label', 'annotator',  'lemma', 'context1', 'context2', 'indices_target_token1', 'indices_target_sentence1', 'indices_target_sentence2', 'indices_target_token2',  'comment', 'label_set', 'non_label', 'dataIDs']\n",
    "    return df[column_order]\n",
    "\n",
    "round_ = 'TRoTR.tsv'\n",
    "df_uses = load_uses()\n",
    "df_instances = load_instances(round_)\n",
    "df_judgments = load_judgments(round_)\n",
    "df = merge_data(df_uses, df_instances, df_judgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3b0e514e-c702-46f3-b13b-46b60d368773",
   "metadata": {},
   "outputs": [],
   "source": [
    "durel_uses = df_uses.copy()\n",
    "#durel_uses['lemma'] = durel_uses['lemma'].apply(lambda x: x.replace(' ', '_').replace(',', ''))\n",
    "durel_uses['description'] = \"\"\n",
    "durel_uses['pos'] = \"\"\n",
    "durel_uses['date'] = \"\"\n",
    "durel_uses['grouping'] = \"\"\n",
    "durel_uses = durel_uses.rename(columns={'dataID': 'identifier', 'indices_target_sentence': 'indexes_target_sentence', 'indices_target_token': 'indexes_target_token'})\n",
    "columns = ['lemma', 'pos', 'date', 'grouping', 'identifier', 'description', 'context', 'indexes_target_token', 'indexes_target_sentence']\n",
    "durel_uses = durel_uses[columns]\n",
    "\n",
    "for lemma in durel_uses[columns].lemma.unique():    \n",
    "    tmp = durel_uses[durel_uses['lemma'] == lemma]\n",
    "    id_quote = re.search('\\(.*\\)', tmp.iloc[0].identifier).group(0).replace(':', ' ')\n",
    "    Path(f'TRoTR/DURel_data/{id_quote}').mkdir(parents=True, exist_ok=True)\n",
    "    tmp.to_csv(f'TRoTR/DURel_data/{id_quote}/uses.tsv', index=False, sep='\\t')\n",
    "\n",
    "    #df['lemma'] = df['lemma'].apply(lambda x: x.replace(' ', '_').replace(',', ''))\n",
    "    durel_judgments = df[df['lemma'] == lemma].copy()    \n",
    "    durel_judgments = durel_judgments.rename(columns={'dataID1': 'identifier1', 'dataID2': 'identifier2', 'label':'judgment'})\n",
    "    columns = ['identifier1', 'identifier2', 'annotator', 'judgment', 'comment', 'lemma']\n",
    "    durel_judgments = durel_judgments[columns]\n",
    "    durel_judgments.to_csv(f'TRoTR/DURel_data/{id_quote}/judgments.tsv', index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
